{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import urllib2\n",
    "import datetime\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of URL we have to find:\n",
    "https://en.wikipedia.org/wiki/Talk:Antifa_(United_States)/Archive_4#Request_for_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The URL we will use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/w/index.php?search=RfC%3A_Should_%7B%7Binfobox_criminal%7D%7D_be_used_in_this_article%3F+prefix%3ATalk%3AAloysius+Stepinac&title=Special:Search&profile=default&fulltext=1&searchToken=mhp1jccsq7yxe23x5n6z1r2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the file names\n",
    "diff_folder_dir = \"/Users/jane/rfc-analysis/diffs/\"\n",
    "diff_files = []\n",
    "for (dirpath, dirnames, filenames) in walk(diff_folder_dir):\n",
    "    diff_files.extend(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_rfc_url(rfc_info):\n",
    "    rfc_anchor = rfc_info[\"parsed_rfc_title\"]\n",
    "    rfc_page_title = rfc_info[\"page_title\"]\n",
    "    \n",
    "    rfc_url_candidates = find_rfc_url_from_archive(rfc_anchor, rfc_page_title)\n",
    "    return rfc_url_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_rfc_url_from_archive(rfc_anchor, rfc_page_title):\n",
    "    \"\"\" make sure to check for duplicates \"\"\"\n",
    "    url_candidates = get_rfc_url_candidates(rfc_anchor, rfc_page_title)\n",
    "    narrowed_url_candidates = set()\n",
    "    if len(url_candidates) > 0:\n",
    "        # make sure to cover cases when there are multiple candidates\n",
    "        for c in url_candidates:\n",
    "            anchor_section = c.split(\"#\")[-1]\n",
    "#             print anchor_section + '- ' + rfc_anchor\n",
    "            if anchor_section.strip() == rfc_anchor.strip():\n",
    "                # include only the exact same ones\n",
    "                narrowed_url_candidates.add(c.strip())\n",
    "                return list(narrowed_url_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rfc_url_candidates(rfc_anchor, rfc_page_title):\n",
    "    archive_search_url = \"https://en.wikipedia.org/w/index.php?search=\" + rfc_anchor.strip() + \"&prefix=\" + rfc_page_title + \"&title=Special:Search&profile=default&fulltext=1\"\n",
    "    archive_search_url = archive_search_url.replace(\" \", \"_\").encode('utf-8') # have to do this because of the page title\n",
    "    archive_search_url = get_rid_of_links(archive_search_url)\n",
    "    archive_search_page = urllib2.urlopen(archive_search_url)\n",
    "    soup = BeautifulSoup(archive_search_page, 'lxml')\n",
    "    try:\n",
    "#         candidates = [c.find(class_='searchalttitle').a['href'].strip() for c in soup.find_all(class_='mw-search-result-heading')]\n",
    "        candidates = []\n",
    "        for c in soup.find_all(class_='mw-search-result-heading'):\n",
    "#             print c\n",
    "            if c.find(class_='searchalttitle') is not None:\n",
    "                candidates.append(c.find(class_='searchalttitle').a['href'].strip())\n",
    "        return candidates\n",
    "    except Exception:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rid_of_links(keyword):\n",
    "    # function for handling cases like ==Proposed redirect to [[Occupy Wall Street#Anthony Bologna]]==\n",
    "    # https://en.wikipedia.org/wiki/Talk:Pepper_spraying_of_the_Occupy_Wall_Street_demonstrators#Proposed_redirect_to_Occupy_Wall_Street#Anthony_Bologna\n",
    "    keyword = keyword.replace(\"[[\", \"\").replace(\"]]\",\"\")\n",
    "    return keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rid_of_links(\"==Proposed redirect to [[Occupy Wall Street#Anthony Bologna]]==\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function that iterates over the files and tries to find the corresponding url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_batch_urls(rfc_files, folder_dir, single_url_found, multiple_url_found, url_unfound, file_name_to_replace):\n",
    "    for i, file_name in enumerate(rfc_files):\n",
    "        if \".json\" in file_name:\n",
    "            rfc_id = file_name.replace(file_name_to_replace, \"\").replace(\".json\", \"\")\n",
    "            if rfc_id not in single_url_found.keys() and rfc_id not in multiple_url_found.keys():\n",
    "                path = folder_dir + file_name\n",
    "                with open(path) as file:\n",
    "                    rfc_info = json.load(file)\n",
    "                if \"parsed_rfc_title\" in rfc_info:\n",
    "                    rfc_id = rfc_info[\"id\"]\n",
    "                    try:\n",
    "                        urls = find_rfc_url(rfc_info)\n",
    "                        if urls is None:\n",
    "                            url_unfound.add(rfc_id)\n",
    "                        else:\n",
    "                            if len(urls) > 1:\n",
    "                                multiple_urls_found[rfc_id] = urls\n",
    "                            else:\n",
    "                                single_url_found[rfc_id] = urls[0]\n",
    "                    except urllib2.HTTPError:\n",
    "                        url_unfound.add(rfc_id)\n",
    "    return single_url_found, multiple_url_found, url_unfound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIFFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_url_found, multiple_url_found, url_unfound = find_batch_urls(diff_files, diff_folder_dir, single_url_found, multiple_url_found, url_unfound, \"diff_added_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfound retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/jane/rfc-analysis/diff_url_unfound.json\") as file:\n",
    "    unfound_retry = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf = {}\n",
    "mf = {}\n",
    "uf = set()\n",
    "unfound_retry_files = []\n",
    "\n",
    "for id in unfound_retry:\n",
    "    unfound_retry_files.append(\"diff_added_\" + id + \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf, mf, uf = find_batch_urls(unfound_retry_files, diff_folder_dir, sf, mf, uf, \"diff_added_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "#### There were no issues due to same section title. \n",
    "#### In total 6,712 new urls were found from stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(single_url_found.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(multiple_url_found.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/jane/rfc-analysis/diff_url_single.json\", \"w\") as file:\n",
    "    json.dump(single_url_found, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/jane/rfc-analysis/diff_url_unfound.json\", \"w\") as file:\n",
    "    json.dump(list(url_unfound), file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
